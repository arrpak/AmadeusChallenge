{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[EN] Week 01 - Introduction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arrpak/AmadeusChallenge/blob/master/%5BEN%5D_Week_01_Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlqOAQmQGXOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip -q install eli5==0.8.1\n",
        "!pip -q install spacy==2.0.18\n",
        "!pip -q install keras==2.2.4\n",
        "!python -m spacy download en\n",
        "\n",
        "!wget -O imdb.zip -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1vrQ5czMHoO3pEnmofFskymXMkq_u1dPc\"\n",
        "!unzip imdb.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqbkG4qoS0AB",
        "colab_type": "text"
      },
      "source": [
        "# Text Classification\n",
        "\n",
        "Let's start with probably the simplest task in NLP - sentiment analysis.\n",
        "\n",
        "We are going to classify IMDB review on positives and negatives.\n",
        "\n",
        "The dataset was taken from http://ai.stanford.edu/~amaas/data/sentiment/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R9e3pctHIV2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv(\"train.tsv\", delimiter=\"\\t\")\n",
        "test_df = pd.read_csv(\"test.tsv\", delimiter=\"\\t\")\n",
        "\n",
        "print('Train size = {}'.format(len(train_df)))\n",
        "print('Test size = {}'.format(len(test_df)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaQRrVsSzloH",
        "colab_type": "text"
      },
      "source": [
        "Let's look through the texts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Js5kqmWMGqN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head train.tsv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ1A1jD8yd58",
        "colab_type": "text"
      },
      "source": [
        "## The very first classifier\n",
        "\n",
        "We-e-ell, it looks quite straightforward. The review \"I simply love this movie\" is positive and when a guy says \"I would almost recommend this film just so people can truly see a 1/10\" he obviously didn't like it.\n",
        "\n",
        "Is it really that simple?\n",
        "\n",
        "Check your ability to classify the texts! Find the words and phrases which can be used as an indicator of a positive or a negative review. Add them to the list below.\n",
        "\n",
        "Our first classifier will predict positive sentiment when the number of positive words in the text is greater than the negative ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DywUCyMLr_TD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Starting classification! { vertical-output: true, display-mode: \"form\" }\n",
        "positive_words = 'love', 'great', 'best', 'wonderful' #@param {type:\"raw\"}\n",
        "negative_words = 'worst', 'awful', '1/10', 'crap' #@param {type:\"raw\"}\n",
        "\n",
        "positives_count = test_df.review.apply(lambda text: sum(word in text for word in positive_words))\n",
        "negatives_count = test_df.review.apply(lambda text: sum(word in text for word in negative_words))\n",
        "is_positive = positives_count > negatives_count\n",
        "correct_count = (is_positive == test_df.is_positive).values.sum()\n",
        "\n",
        "accuracy = correct_count / len(test_df)\n",
        "\n",
        "print('Test accuracy = {:.2%}'.format(accuracy))\n",
        "if accuracy > 0.71:\n",
        "    from IPython.display import Image, display\n",
        "    display(Image('https://i.ibb.co/M5wrxZd/Implemented-a-better-than-random-classifier.png', width=500))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo8HRABxv1kW",
        "colab_type": "text"
      },
      "source": [
        "**Task** Find good keywords and phrases and achieve at least 71% accuracy on the test set (and don't forget to check the evaluation code of the classifier)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaIrBClMUHZB",
        "colab_type": "text"
      },
      "source": [
        "## A bit more serious classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVN_edeDT6IM",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "**Task** Is there anyone who likes these `<br /><br />` inside the reviews? Write a regex that would delete 'em."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09OkUmtde6ny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "pattern = re.compile(<implement it>)\n",
        "\n",
        "print(train_df['review'].iloc[3])\n",
        "print(pattern.subn(' ', train_df['review'].iloc[3])[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO6D9NuMi4II",
        "colab_type": "text"
      },
      "source": [
        "Apply it with `apply` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LTwQqs_hD-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df['review'] = train_df['review'].apply(lambda text: pattern.subn(' ', text)[0])\n",
        "test_df['review'] = test_df['review'].apply(lambda text: pattern.subn(' ', text)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGxzf4oXmXqw",
        "colab_type": "text"
      },
      "source": [
        "And it's machine learning time, finally!\n",
        "\n",
        "First of all, we have to decide how to represent the text. 'Cos you have to explain to the computer that the texts are not just the sequences of bytes.\n",
        "\n",
        "The simplest representation is bag-of-words.\n",
        "\n",
        "Let's build a very-very large dictionary - list of all words in the train set, for instance. Then the text can be translated to the vector with the length equal to our dictionary size. Each element of the vector shows how many times the corresponding word appeared in the text:\n",
        "\n",
        "![bow](https://raw.githubusercontent.com/DanAnastasyev/DeepNLP-Course/master/Week%2001/Images/BOW.png)\n",
        "\n",
        "Does it sound hard? Actually, we don't even need to implement it - everything works out of the box in sklearn `CountVectorizer`.\n",
        "\n",
        "It has following signature:\n",
        "\n",
        "```python\n",
        "CountVectorizer(input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern=r'(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class â€˜numpy.int64'>)\n",
        "```\n",
        "\n",
        "Geeze, that were lots of parameters.\n",
        "\n",
        "Let's pay attention to `lowercase=True` and `max_df=1.0, min_df=1, max_features=None` parameters. They are mainly about the size of the dictionary. \n",
        "\n",
        "The first one is about word's case. That is, do we need to consider, e.g. \"The\" and \"the\" as different words or similar? And what about \"Smith\" vs \"smith\"? In fact, we've just faced our first trade-off: are we willing to reduce dictionary size, but treat some different words as a single.\n",
        "\n",
        "The next group is about statistical dictionary reduction. We can use only words that are not too infrequent (e.g. `min_df=5` says that we need only words that appeared at least 5 times in the train set), and not too frequent (`max_df=0.2` says that words with frequency more than 0.2 should be ignored).\n",
        "\n",
        "Let's start with a simple example to understand better how it works:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Odnum4iyGDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "dummy_data = ['The movie was excellent',\n",
        "              'the movie was awful']\n",
        "\n",
        "dummy_matrix = vectorizer.fit_transform(dummy_data)\n",
        "\n",
        "print(dummy_matrix.toarray())\n",
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3zC1ItWybVc",
        "colab_type": "text"
      },
      "source": [
        "*Question: how does the vectorizer find the word's boundaries? Pay attention to `token_pattern=r'(?u)\\b\\w\\w+\\b'` - how does such regex work?*\n",
        "\n",
        "Check it on the real data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ccd2gaCdQq2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(train_df['review'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_JC9n6C0bFR",
        "colab_type": "text"
      },
      "source": [
        "We can explore the words that appear in the dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stV4ICO3mKsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oesbuQ9g0krj",
        "colab_type": "text"
      },
      "source": [
        "And that's how it can be used to transform the text to its vector:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUWtDWcp0g7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer.transform([train_df['review'].iloc[3]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZCRef3pyCRC",
        "colab_type": "text"
      },
      "source": [
        "Exactly what we were going to get: a vector with 74849 elements (*check how many words are there in the dictionary*) and 207 non-zero elements. It's stored in the sparse matrix for exactly this reason: it's inefficient to create a dense matrix that has almost 75 thousand zeros when we can just store the non-zero positions and corresponding elements.\n",
        "\n",
        "### Classification\n",
        "\n",
        "Wait, and what should we do next with all these zeros (and 207 non-zeros, meh)? Well, actually we are gonna do the same thing we did in the very first assignment. Some words are positives, others are negatives. This time let's use ML to decide which is which!\n",
        "\n",
        "![bow with weights](https://github.com/DanAnastasyev/DeepNLP-Course/raw/master/Week%2001/Images/BOW_weights.png)\n",
        "\n",
        "Look, we have the vector with 5 ones. \"the\", \"movie\" and \"was\" are obviously neutral. \"Good\" should be positive, \"ridiculously\" - probably negative. And the whole sentence should be positive. So the ML algorithm has to find the weights for each word. That is, larger positive weights would be given to the most certainly positive words.\n",
        "\n",
        "For instance, let's consider the two sentences set:\n",
        "```\n",
        "1   The movie was excellent\n",
        "0   the movie was awful\n",
        "```\n",
        "\n",
        "You can probably guess that `+1` weight should be assigned to the `excellent` word, `-1` to the `awful` and every other word would have zero weight.\n",
        "\n",
        "We are gonna use a linear model which will find the weights.\n",
        "\n",
        "Let's try Logistic regression on our super-small set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6WVgK4LtUn2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "dummy_data = ['The movie was excellent',\n",
        "              'the movie was awful']\n",
        "dummy_labels = [1, 0]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "model = Pipeline([\n",
        "    ('vectorizer', vectorizer),\n",
        "    ('classifier', classifier)\n",
        "])\n",
        "\n",
        "model.fit(dummy_data, dummy_labels)\n",
        "\n",
        "for word, coef in zip(vectorizer.get_feature_names(), classifier.coef_[0]):\n",
        "    print(word, coef, sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9Y-kq-tv-XY",
        "colab_type": "text"
      },
      "source": [
        "Well, it did exactly what we expected.\n",
        "\n",
        "It's time to try it on the real data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvxHIIbSiUXq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(train_df['review'], train_df['is_positive'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d3BBV_uUu-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def eval_model(model, test_df):\n",
        "    preds = model.predict(test_df['review'])\n",
        "    print('Test accuracy = {:.2%}'.format(accuracy_score(test_df['is_positive'], preds)))\n",
        "    return accuracy_score(test_df['is_positive'], preds)\n",
        "    \n",
        "accuracy = eval_model(model, test_df)\n",
        "\n",
        "assert accuracy > 0.86, 'Guess, something went wrong'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pnmmz3u71ctO",
        "colab_type": "text"
      },
      "source": [
        "Wow, we got some results! And it even better than the score of our first classifier!\n",
        "\n",
        "Right now we should move to the most crucial part of the whole NLP - analysis of the model. Luckily, with logistic regression it's extremely simple.\n",
        "\n",
        "We'll use great library [ELI5](https://eli5.readthedocs.io/en/latest/) for it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W1Ngl-aVuYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import eli5\n",
        "\n",
        "eli5.show_weights(classifier, vec=vectorizer, top=40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VgCE9tDk-aO",
        "colab_type": "text"
      },
      "source": [
        "What were we talking about?! Our model learnt to distinguish positive and negative words! No magic - just statistics!\n",
        "\n",
        "We can also check its work on specific reviews:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-4sVWBOWJpo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Positive' if test_df['is_positive'].iloc[1] else 'Negative')\n",
        "eli5.show_prediction(classifier, test_df['review'].iloc[1], vec=vectorizer, \n",
        "                     targets=['positive'], target_names=['negative', 'positive'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU-2pbrVNktj",
        "colab_type": "text"
      },
      "source": [
        "You see, some words have a positive connotation, others are negative. You can see which is which (and if you hover your cursor over a specific word, you'll see its weight). The review above contains mostly positive words, and the model predicts correctly that the review is positive.\n",
        "\n",
        "What are we going to see in a negative review?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoZhtlYlW-xG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Positive' if test_df['is_positive'].iloc[0] else 'Negative')\n",
        "eli5.show_prediction(classifier, test_df['review'].iloc[0], vec=vectorizer, \n",
        "                     targets=['positive'], target_names=['negative', 'positive'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNNUqZvplhAC",
        "colab_type": "text"
      },
      "source": [
        "Well, not bad, isn't it?\n",
        "\n",
        "And the most essential: let's try to understand when and why the model makes wrong predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yf9ZzS8fXKFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "preds = model.predict(test_df['review'])\n",
        "incorrect_pred_index = np.random.choice(np.where(preds != test_df['is_positive'])[0])\n",
        "\n",
        "eli5.show_prediction(classifier, test_df['review'].iloc[incorrect_pred_index],\n",
        "                     vec=vectorizer, targets=['positive'], target_names=['negative', 'positive'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4J-V5VuO-iv",
        "colab_type": "text"
      },
      "source": [
        "What do you think can be improved?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbFXKNrngP46",
        "colab_type": "text"
      },
      "source": [
        "## Additional Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz7GSFzIlv4V",
        "colab_type": "text"
      },
      "source": [
        "### Tf-idf\n",
        "\n",
        "Our model doesn't take into account any information about the words except their count in the specific text.\n",
        "\n",
        "What if we want to help it? We already discussed that some words are more meaningful than others. And the words that appeared in most train texts are probably not too meaningful for our model. On the other hand, some words can be found just in a handful of texts.\n",
        "\n",
        "Let's explain to the model that it should pay more attention to the second type of words rather than the first ones.\n",
        "\n",
        "The simplest way to do it is to apply *tf-idf* weighting:\n",
        "$$\\text{tf-idf}(t, d) = \\text{tf}(t, d) \\times \\text{idf}(t)$$\n",
        "\n",
        "*tf* (term-frequency) is the count of occurrences of the word `t` in specific text `d`. This is the count yielded by `CountVectorizer`.\n",
        "\n",
        "*idf* (inverse document-frequency) is term that is inversely proportional to the number of texts with the given word. It can be calculated this way:\n",
        "$$\\text{idf}(t) = \\text{log}\\frac{1 + n_d}{1 + n_{d(t)}} + 1$$\n",
        "where $n_d$ is the whole number of texts and $n_{d(t)}$ is the number of texts with the word `t`.\n",
        "\n",
        "Well, you probably don't even need to remember the formula above. Just think about it as an unsupervised way to calculate additional word weight.\n",
        "\n",
        "To use it, just replace `CountVectorizer` with `TfidfVectorizer`.\n",
        "\n",
        "**Task** Use `TfidfVectorizer`, Luke! Check the quality of the model and look to its mistakes. Find the sentences that it started to annotate correctly compared to the previous model and find newly added mistakes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3DjjiJglvT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "<use TfidfVectorizer model>\n",
        "\n",
        "accuracy = eval_model(model, test_df)\n",
        "\n",
        "assert accuracy > 0.88, 'Guess, something went wrong'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xe_CJxQ0tFP9",
        "colab_type": "text"
      },
      "source": [
        "### Word N-grams\n",
        "\n",
        "Up to this moment, we considered texts as a simple bag-of-words. But, hey, there is a difference between `good movie` and `not good movie`!\n",
        "\n",
        "It's time to provide our model with such information. We are going to extract word bigrams. For instance, `not good movie` we will split into two bigrams: `not good` and `good movie`. Additionally, we will extract unigrams like we did before: `not`, `good` and `movie`.\n",
        "\n",
        "In both Vectorizers you can find `ngram_range=(n_1, n_2)` parameter. It's about the n-grams: `ngram_range=(1, 2)` means that we are going to extract uni- and bigrams.\n",
        "\n",
        "**Task** Increase the ngram range and evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDpdrT0HuKYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "<implement it>\n",
        "\n",
        "accuracy = eval_model(model, test_df)\n",
        "assert accuracy > 0.865, 'Guess, something went wrong'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrBoThj6wl2F",
        "colab_type": "text"
      },
      "source": [
        "### Symbol N-grams\n",
        "\n",
        "Symbol n-grams give us a quick-and-dirty way to learn useful subwords without any linguistics.\n",
        "\n",
        "For example, the word `badass` can be represented as a set of following trigrams:  \n",
        "`##b #ba bad ada das ass ss# s##`\n",
        "\n",
        "So interpretable, isn't it?\n",
        "\n",
        "And again, it's very easy to implement. Just use `analyzer='char'` in your favourite Vectorizer and decide (or find using cross-validation) `ngram_range`.\n",
        "\n",
        "**Task** Implement this stuff and don't forget to visualise it in ELI5 (it gives very nice results, really)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFaWmUrGyY3n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "<implement it>\n",
        "\n",
        "accuracy = eval_model(model, test_df)\n",
        "assert accuracy > 0.875, 'Guess, something went wrong'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fZ6I8mN0VPU",
        "colab_type": "text"
      },
      "source": [
        "## Linguistic-Driven Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkywIvbp4N-L",
        "colab_type": "text"
      },
      "source": [
        "### Lemmatization\n",
        "\n",
        "Do we really need to distinguish the forms of a single word? For instance, why single and plural forms of a word have different weights.\n",
        "\n",
        "**Task** Find forms of a word with different semantics according to model (based on the model weights).\n",
        "\n",
        "To do something with this stuff, we can apply another nice library - [spacy](https://spacy.io)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5CTTdtxI5yv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "nlp = spacy.load('en', disable=['parser'])\n",
        "\n",
        "docs = [doc for doc in nlp.pipe(train_df.review.values[:50])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXs8Ia_bqeS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for token in docs[0]:\n",
        "    print(token.text, token.lemma_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVXJr_xxqlPK",
        "colab_type": "text"
      },
      "source": [
        "You can process the whole corpus (the set of available texts), but to make things faster I preprocessed everything for you.\n",
        "\n",
        "To download preprocessed documents you'll have to authorize the request to Google Drive API, sorry."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRdS0CM2rSzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "downloaded = drive.CreateFile({'id': '1d1-5FwxK53ePwygNWeG7jhsOWZbi5HOv'})\n",
        "downloaded.GetContentFile('train_docs.pkl')\n",
        "\n",
        "downloaded = drive.CreateFile({'id': '1MMOY477t965G0C5DtXeREVp0X85UaNq5'})\n",
        "downloaded.GetContentFile('test_docs.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZNz7E5JqrWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open('train_docs.pkl', 'rb') as f:\n",
        "    train_docs = pickle.load(f)\n",
        "    \n",
        "with open('test_docs.pkl', 'rb') as f:\n",
        "    test_docs = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3Al4i2ngcj0",
        "colab_type": "text"
      },
      "source": [
        "The preprocessed data consists of the list of word-lemma pairs (the first element in the tuple):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEeEd_E9gYPx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_docs[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IN_utSoAgx1Y",
        "colab_type": "text"
      },
      "source": [
        "and NER information (we'll discuss it a bit later):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ty8pZNm9gwDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_docs[0][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Pfup3O5r30m",
        "colab_type": "text"
      },
      "source": [
        "**Task** Apply classifier to the lemmatized texts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDM4O3zMN2JX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59BKhHPGN1oS",
        "colab_type": "text"
      },
      "source": [
        "### Stemming\n",
        "\n",
        "More computationally effective (and much dirtier) way to normalize words is to use stemming. It's dumb and doesn't take into account the context of the words but this is why it's so efficient.\n",
        "\n",
        "Basically, it's just a set of rules how to process a word to obtain its stem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr0w_hVyrqFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "print(stemmer.stem('become'))\n",
        "print(stemmer.stem('becomes'))\n",
        "print(stemmer.stem('became'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxGSEqHNsfHy",
        "colab_type": "text"
      },
      "source": [
        "**Task** Try to replace lemmas with stems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydpDm9oSN3Ja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1NBpkwuspBX",
        "colab_type": "text"
      },
      "source": [
        "### NER\n",
        "\n",
        "NER stands to Named-Entity Recognition. It's a process of finding entities in texts. Such as:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xki8Maf9MrVY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "displacy.render(docs[0], style='ent', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWiVgYgKjaQT",
        "colab_type": "text"
      },
      "source": [
        "The entities are described [here](https://spacy.io/api/annotation#named-entities).\n",
        "\n",
        "The preprocessed data contains the entity type and its coordinates in the original text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UX1Avy_j-pX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ner_data = train_docs[0][1]\n",
        "print(train_df.review.iloc[0][ner_data[0][0]: ner_data[0][1]], ner_data[0][2])\n",
        "print(train_df.review.iloc[0][ner_data[2][0]: ner_data[2][1]], ner_data[2][2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGhcnuZSteHc",
        "colab_type": "text"
      },
      "source": [
        "Are we sure that a guy like Depp has semantic connotation? Or is it just our classifier learnt that some actors appear in positive reviews more frequently than in negatives?\n",
        "\n",
        "**Task** Collect the list of entities in the preprocessed data and find which of them are strongly positive or strongly negative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Olz-CAA1kndL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOcGiqEekoyF",
        "colab_type": "text"
      },
      "source": [
        "**Task** Remove entities (or rather some of them) from the texts. Check the classifier on normalized texts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPO-jYQJjVJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2fHA70b0zEZ",
        "colab_type": "text"
      },
      "source": [
        "## Deep Learning Time!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1VCrM671XO5",
        "colab_type": "text"
      },
      "source": [
        "The tiny fraction of you who still reading this is probably wondering: why we didn't use any deep learning in the DL for NLP course.\n",
        "\n",
        "Let's use somewhat standard texts classification model - convolutional neural network over word embeddings.\n",
        "\n",
        "We are gonna dig into what it is in the following notebooks. Right now we'll just use it :)\n",
        "\n",
        "### Preprocessing\n",
        "Well, things are not that simple, actually... We have to prepare the texts for the neural network.\n",
        "\n",
        "Every text should be represented as a sequence of words - this the main difference between this and bag-of-words representation.\n",
        "\n",
        "And it's not that simple to obtain the sequence. First of all, the raw texts have to be *tokenized*. This is the process of splitting text into tokens.\n",
        "\n",
        "Actually, spacy did it for us already:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG1eGmuKNkrP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_tokenized_texts = [[token for token, _ in doc[0]] for doc in train_docs]\n",
        "test_tokenized_texts = [[token for token, _ in doc[0]] for doc in test_docs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I4kWRc0O57R",
        "colab_type": "text"
      },
      "source": [
        "Compare untokenized text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ibi42T7JO0yx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.review.iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNvSCm_kO85P",
        "colab_type": "text"
      },
      "source": [
        "With tokenized one:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYwzdJR9OyUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "' '.join(train_tokenized_texts[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT1mCv2dOZfo",
        "colab_type": "text"
      },
      "source": [
        "Well, we still need to know the sequence length in advance. To speed up the process we can cut too long reviews. Let's use the histogram to understand where to cut: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O477ZV1t1WIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "_, _, hist = plt.hist([len(text) for text in train_tokenized_texts], bins='auto')\n",
        "hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXO4xi0u5m8l",
        "colab_type": "text"
      },
      "source": [
        "Another problem: the words have to be enumerated. Actually, it's quite similar to the stuff that Vectorizers did for us. We have to build the dictionary (again, remember?) which will map each word to its index.\n",
        "\n",
        "**Task** Choose the minimum number of word occurrences which we consider interesting. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIMGE7L-55fs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "def build_word2idx(tokenized_texts, min_df):\n",
        "    words_counter = Counter((word.lower() for text in tokenized_texts for word in text))\n",
        "\n",
        "    word2idx = {\n",
        "        '<pad>': 0,\n",
        "        '<unk>': 1\n",
        "    }\n",
        "    for word, count in words_counter.most_common():\n",
        "        if count < min_df:\n",
        "            break\n",
        "\n",
        "        word2idx[word] = len(word2idx)\n",
        "    return word2idx\n",
        "    \n",
        "    \n",
        "word2idx = build_word2idx(train_tokenized_texts, <choose me>)\n",
        "print('Words count:', len(word2idx))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv7mf-dTFQn3",
        "colab_type": "text"
      },
      "source": [
        "Let's check what we just did:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "471Iov24FQJ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTptlmd1yD3J",
        "colab_type": "text"
      },
      "source": [
        "Using `word2idx` and `max_text_len` constant deduced by the histogram we are able to convert texts to a matrix.\n",
        "\n",
        "Wait, what the hell matrix am I talking about?\n",
        "\n",
        "Remember, the Vectorizers converted list of texts to a matrix in the following way:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "& \\text{the movie was excellent} \\\\\n",
        "& \\text{the movie was extremely awful}\n",
        "\\end{align}\n",
        "\\to\n",
        " \\begin{pmatrix}\n",
        "  1 & 1 & 1 & 1 & 0 & 0 \\\\\n",
        "  1 & 1 & 1 & 0 & 1 & 1\n",
        " \\end{pmatrix}\n",
        "$$\n",
        "\n",
        "The first row in the matrix corresponds to the first sentence, the second... well, you already know, yeah.\n",
        "\n",
        "The first column says how many times `the` appears in this specific sentence, the last is about how many times `awful` appeared, and so on.\n",
        "\n",
        "Such representation gives us a fixed-length vector without saving of word-order.\n",
        "\n",
        "Now we are going to build variable (almost) length vector for each text. Its length would be equal to the minimum between the text length and `max_text_len` constant.\n",
        "\n",
        "We are going to write in each position of the vector index of the corresponding word in the text. And the `word2idx` mapping will give us the index.\n",
        "\n",
        "For instance, for these two sentences above we'll have `word2idx`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qcb2cou0P5H9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dummy_texts = ['the movie was excellent', 'the movie was extremely awful']\n",
        "dummy_tokenized_texts = [text.split() for text in dummy_texts]\n",
        "\n",
        "dummy_word2idx = build_word2idx(dummy_tokenized_texts, 0)\n",
        "\n",
        "dummy_word2idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7HEOjs0P10b",
        "colab_type": "text"
      },
      "source": [
        "Basically, its the same mapping as in Vectorizer but for the first two elements.\n",
        "\n",
        "We need `'<unk>'` to be able to map unknown words (e.g. words with frequency lower than the `min_df` threshold) to some index.\n",
        "\n",
        "And we need `'<pad>'` for... well, let's see how the sentences can be converted:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "& \\text{the movie was excellent} \\\\\n",
        "& \\text{the movie was extremely awful}\n",
        "\\end{align}\n",
        "\\to\n",
        " \\begin{pmatrix}\n",
        "  2 & 3 & 4 & 5 & 0 \\\\\n",
        "  2 & 3 & 4 & 6 & 7\n",
        " \\end{pmatrix}\n",
        "$$\n",
        "\n",
        "We need the `'<pad>'` symbol to be able to build a matrix from the variable-length vectors. We just pad the shorter vectors with zeros and hope that neural net will figure out that it should not deal anyhow with this zeros.\n",
        "\n",
        "The conversion of each text can be performed in the following way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRZ1miyMQc3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[dummy_word2idx.get(token, 1) for token in dummy_tokenized_texts[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tuHUHPzQcIN",
        "colab_type": "text"
      },
      "source": [
        "The get method of our mapping would return 1 (index of `'<unk>'`)  each time it meets an out of mapping word.\n",
        "\n",
        "To make use of the paddings, you can use the following trick: fill initial matrix with zeros (`np.zeros(shape)`) and replace zeros with real elements:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awxMBl_XR4mE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dummy_data = np.zeros((2, 5), dtype=np.int)\n",
        "\n",
        "dummy_data[0, :len(dummy_tokenized_texts[0])] = [dummy_word2idx.get(token, 1) for token in dummy_tokenized_texts[0]]\n",
        "\n",
        "dummy_data[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfPa0m-WR3ax",
        "colab_type": "text"
      },
      "source": [
        "**Task** Convert the whole train and test sets to matrices in the similar way as described above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPP0cYdJ5VkE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert(texts, word2idx, max_text_len):\n",
        "    data = np.zeros((len(texts), max_text_len), dtype=np.int)\n",
        "    \n",
        "    <implement it>\n",
        "\n",
        "    return data\n",
        "\n",
        "X_train = convert(train_tokenized_texts, word2idx, 1000)\n",
        "X_test = convert(test_tokenized_texts, word2idx, 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYb-p5ioyLUZ",
        "colab_type": "text"
      },
      "source": [
        "### Classification\n",
        "\n",
        "Let's finally run the training of the model.\n",
        "\n",
        "In keras to train a model you have to:\n",
        "1. Define the model, e.g.:\n",
        "```python \n",
        "model = Sequential()\n",
        "model.add(Dense(1, activation='sigmoid', input_dim=NUM_WORDS))\n",
        "```\n",
        "2. Set the loss function and optimizer (and metrics, if you want):\n",
        "```python\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "```\n",
        "\n",
        "3. Run the training:\n",
        "```python\n",
        "model.fit(X_train, y_train, \n",
        "             batch_size=32, epochs=3,\n",
        "             validation_data=(X_test, y_test))\n",
        "```\n",
        "\n",
        "In NLP you usually deal with classification tasks so you have to know three loss functions:\n",
        "\n",
        "*   **categorical_crossentropy** - for multiclass classification with one-hot encoding vectors as the labels\n",
        "*   **sparse_categorical_crossentropy** - like the previous one but with indices of correct classes, not one-hot encoding vectors\n",
        "*   **binary_crossentropy** - for binary classification\n",
        "\n",
        "As an optimizer you will most probably use `adam`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDjri3167vFf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=len(word2idx), output_dim=64, input_shape=(X_train.shape[1],)),\n",
        "    Conv1D(filters=128, kernel_size=3, padding='valid', activation='relu', strides=1),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw93gTmq8gZl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(X_train, train_df.is_positive, batch_size=128, epochs=5, \n",
        "          validation_data=(X_test, test_df.is_positive))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBGdVRQTyynD",
        "colab_type": "text"
      },
      "source": [
        "Let's evaluate the model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiHaazQ8WHxx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss, accuracy = model.evaluate(X_test, test_df.is_positive)\n",
        "\n",
        "assert accuracy > 0.86"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-MHfe3by8JW",
        "colab_type": "text"
      },
      "source": [
        "## Bits of Maths\n",
        "\n",
        "We used logistic regression so many times today. Let's go over how it works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZBLLlHt8s5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(42)\n",
        "w, X, y = np.random.random(10), np.random.random((11, 10)), 2 * (np.random.randint(0, 2, 11) - 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VFuK-ws8wdO",
        "colab_type": "text"
      },
      "source": [
        "First of all, it's simple linear function with sigmoid activation:\n",
        "\n",
        "$$h_w(X) = \\sigma(X w),$$\n",
        "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
        "\n",
        "Calculate it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVd7ttkozkFJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward(X, w):\n",
        "    \"\"\" Function that calcs h(X, w)\n",
        "    X - matrix (n, m)\n",
        "    w - vector (m,)\n",
        "    \"\"\"\n",
        "    <calc h>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpHJDVJ9whpd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert np.allclose(forward(X, w), [0.93470525, 0.86230993, 0.89161691, 0.89640013, 0.95225746, 0.94261494, \n",
        "                                   0.83213636, 0.94034399, 0.9093287, 0.9004457, 0.94200167])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It8mydxfzr-Q",
        "colab_type": "text"
      },
      "source": [
        "Its loss function looks this way:\n",
        "$$J(w)  = \\frac{1}{m} \\left(-y^T \\text{log}(h_w) - (1-y)^T \\text{log}(1 - h_w)\\right)$$\n",
        "\n",
        "To use gradient descent optimization, we need to calculate gradient\n",
        "$\\frac{\\partial J(w)}{\\partial w}$.\n",
        "\n",
        "**Task** Implement them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjLlb4ux0oJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(X, w, y):\n",
        "    \"\"\" Loss function\n",
        "    X - matrix (n, m)\n",
        "    w - vector (m,)\n",
        "    y - vector (n,)\n",
        "    \"\"\"\n",
        "    <calc loss>\n",
        "\n",
        "\n",
        "def gradient(X, w, y):\n",
        "    \"\"\" Loss function gradient over w\n",
        "    X - matrix (n, m)\n",
        "    w - vector (m,)\n",
        "    y - vector (n,)\n",
        "    \"\"\"\n",
        "    <calc grad>\n",
        "\n",
        "\n",
        "print('loss = ', loss(X, w, y))\n",
        "print('gradient = ', gradient(X, w, y))\n",
        "\n",
        "assert gradient(X, w, y).shape == w.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuxhtK1m9Jal",
        "colab_type": "text"
      },
      "source": [
        "Check yourself: compare your results with gradient calculated using finite differences:\n",
        "\n",
        "$$[\\nabla f(x)]_i \\approx \\frac{f(x + \\varepsilon \\cdot e_i) - f(x)}{\\varepsilon}$$\n",
        "\n",
        "where $e_i = (0, ... , 0, 1, 0, ..., 0), \\varepsilon \\approx 10^{-8}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2FgTg-T9TLO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def grad_finite_diff(func, x, eps=1e-8):\n",
        "    \"\"\"\n",
        "    w - vector (m,)\n",
        "    func - scalar function of w\n",
        "    eps - constant\n",
        "    \"\"\"\n",
        "    x, fval, dnum = x.astype(np.float64), func(x), np.zeros_like(x)\n",
        "    \n",
        "    E = np.eye(x.shape[0])\n",
        "    deltas = np.array([func(x + eps * E[i]) for i in range(x.shape[0])])\n",
        "    return (deltas - fval) / eps\n",
        "\n",
        "\n",
        "mat_grad = gradient(X, w, y)\n",
        "num_grad = grad_finite_diff(lambda w: loss(X, w, y), w)\n",
        "\n",
        "err = np.max(np.abs(mat_grad - num_grad))\n",
        "print('err = ', err, 'ok' if err < 1e-6 else 'the error is too large =(')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zhjPhrM1WR-",
        "colab_type": "text"
      },
      "source": [
        "As a result, we will get following class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6UOJICT020e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogisticRegression:\n",
        "    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True):\n",
        "        self._lr = lr\n",
        "        self._num_iter = num_iter\n",
        "        self._fit_intercept = fit_intercept\n",
        "    \n",
        "    def _add_intercept(self, X):\n",
        "        intercept = np.ones((X.shape[0], 1))\n",
        "        return np.concatenate((intercept, X), axis=1)\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        if self._fit_intercept:\n",
        "            X = self._add_intercept(X)\n",
        "        \n",
        "        self._w = np.zeros(X.shape[1])\n",
        "        \n",
        "        for i in range(self._num_iter):\n",
        "            grad = gradient(X, self._w, y)\n",
        "            self._w -= self._lr * grad\n",
        "    \n",
        "    def predict_prob(self, X):\n",
        "        if self._fit_intercept:\n",
        "            X = self._add_intercept(X)\n",
        "    \n",
        "        return forward(X, self._w)\n",
        "    \n",
        "    def predict(self, X, threshold):\n",
        "        return self.predict_prob(X) >= threshold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A-V-7ST9aIF",
        "colab_type": "text"
      },
      "source": [
        "Let's check it on the simple datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmO0F2cx9X-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import make_moons, load_iris\n",
        "\n",
        "def visualize(model, X, y):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='b', label='0')\n",
        "    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='r', label='1')\n",
        "    plt.legend()\n",
        "\n",
        "    x1_min, x1_max = X[:,0].min(), X[:,0].max(),\n",
        "    x2_min, x2_max = X[:,1].min(), X[:,1].max(),\n",
        "    xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\n",
        "    grid = np.c_[xx1.ravel(), xx2.ravel()]\n",
        "    probs = model.predict_prob(grid).reshape(xx1.shape)\n",
        "    plt.contour(xx1, xx2, probs, [0.5], linewidths=1, colors='black')\n",
        "\n",
        "    \n",
        "iris = load_iris()\n",
        "X = iris.data[:, :2]\n",
        "y = (iris.target != 0) * 1\n",
        "\n",
        "model = LogisticRegression(lr=0.1, num_iter=300000)\n",
        "model.fit(X, y)\n",
        "\n",
        "visualize(model, X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaoQgdAJ9ZaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = make_moons(noise=0.1)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "visualize(model, X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyyTUSCtVth1",
        "colab_type": "text"
      },
      "source": [
        "That's all, folks!"
      ]
    }
  ]
}